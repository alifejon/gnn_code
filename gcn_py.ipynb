{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph Convolutional Networks in PyTorch\n",
    "https://github.com/tkipf/pygcn\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils.py\n",
    "\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import torch\n",
    "import math\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.nn.modules.module import Module\n",
    "\n",
    "\n",
    "def encode_onehot(labels):\n",
    "    classes = set(labels)\n",
    "    classes_dict = {c: np.identity(len(classes))[i, :] for i, c in\n",
    "                    enumerate(classes)}\n",
    "    labels_onehot = np.array(list(map(classes_dict.get, labels)),\n",
    "                             dtype=np.int32)\n",
    "    return labels_onehot\n",
    "\n",
    "\n",
    "def load_data(path=\"data/cora/\", dataset=\"cora\"):\n",
    "    \"\"\"Load citation network dataset (cora only for now)\"\"\"\n",
    "    print('Loading {} dataset...'.format(dataset))\n",
    "\n",
    "    idx_features_labels = np.genfromtxt(\"{}{}.content\".format(path, dataset),\n",
    "                                        dtype=np.dtype(str))\n",
    "    features = sp.csr_matrix(idx_features_labels[:, 1:-1], dtype=np.float32)\n",
    "    labels = encode_onehot(idx_features_labels[:, -1])\n",
    "\n",
    "    # build graph\n",
    "    idx = np.array(idx_features_labels[:, 0], dtype=np.int32)\n",
    "    idx_map = {j: i for i, j in enumerate(idx)}\n",
    "    edges_unordered = np.genfromtxt(\"{}{}.cites\".format(path, dataset),\n",
    "                                    dtype=np.int32)\n",
    "    edges = np.array(list(map(idx_map.get, edges_unordered.flatten())),\n",
    "                     dtype=np.int32).reshape(edges_unordered.shape)\n",
    "    adj = sp.coo_matrix((np.ones(edges.shape[0]), (edges[:, 0], edges[:, 1])),\n",
    "                        shape=(labels.shape[0], labels.shape[0]),\n",
    "                        dtype=np.float32)\n",
    "\n",
    "    # build symmetric adjacency matrix\n",
    "    adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n",
    "\n",
    "    features = normalize(features)\n",
    "    adj = normalize(adj + sp.eye(adj.shape[0]))\n",
    "\n",
    "    idx_train = range(140)\n",
    "    idx_val = range(200, 500)\n",
    "    idx_test = range(500, 1500)\n",
    "\n",
    "    features = torch.FloatTensor(np.array(features.todense()))\n",
    "    labels = torch.LongTensor(np.where(labels)[1])\n",
    "    adj = sparse_mx_to_torch_sparse_tensor(adj)\n",
    "\n",
    "    idx_train = torch.LongTensor(idx_train)\n",
    "    idx_val = torch.LongTensor(idx_val)\n",
    "    idx_test = torch.LongTensor(idx_test)\n",
    "\n",
    "    return adj, features, labels, idx_train, idx_val, idx_test\n",
    "\n",
    "\n",
    "def normalize(mx):\n",
    "    \"\"\"Row-normalize sparse matrix\"\"\"\n",
    "    rowsum = np.array(mx.sum(1))\n",
    "    r_inv = np.power(rowsum, -1).flatten()\n",
    "    r_inv[np.isinf(r_inv)] = 0.\n",
    "    r_mat_inv = sp.diags(r_inv)\n",
    "    mx = r_mat_inv.dot(mx)\n",
    "    return mx\n",
    "\n",
    "\n",
    "def accuracy(output, labels):\n",
    "    preds = output.max(1)[1].type_as(labels)\n",
    "    correct = preds.eq(labels).double()\n",
    "    correct = correct.sum()\n",
    "    return correct / len(labels)\n",
    "\n",
    "\n",
    "def sparse_mx_to_torch_sparse_tensor(sparse_mx):\n",
    "    \"\"\"Convert a scipy sparse matrix to a torch sparse tensor.\"\"\"\n",
    "    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
    "    indices = torch.from_numpy(\n",
    "        np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n",
    "    values = torch.from_numpy(sparse_mx.data)\n",
    "    shape = torch.Size(sparse_mx.shape)\n",
    "    return torch.sparse.FloatTensor(indices, values, shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "\n",
    "\n",
    "class GraphConvolution(Module):\n",
    "    \"\"\"\n",
    "    Simple GCN layer, similar to https://arxiv.org/abs/1609.02907\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super(GraphConvolution, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = Parameter(torch.FloatTensor(in_features, out_features))\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.FloatTensor(out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, input, adj):\n",
    "        support = torch.mm(input, self.weight)\n",
    "        output = torch.spmm(adj, support)\n",
    "        if self.bias is not None:\n",
    "            return output + self.bias\n",
    "        else:\n",
    "            return output\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + ' (' \\\n",
    "               + str(self.in_features) + ' -> ' \\\n",
    "               + str(self.out_features) + ')'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models.py\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, nfeat, nhid, nclass, dropout):\n",
    "        super(GCN, self).__init__()\n",
    "\n",
    "        self.gc1 = GraphConvolution(nfeat, nhid)\n",
    "        self.gc2 = GraphConvolution(nhid, nclass)\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        x = F.relu(self.gc1(x, adj))\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        x = self.gc2(x, adj)\n",
    "        return F.log_softmax(x, dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cora dataset...\n",
      "Epoch: 0001 loss_train: 1.9548 acc_train: 0.1500 loss_val: 1.9546 acc_val: 0.1567 time: 0.2533s\n",
      "Epoch: 0002 loss_train: 1.9429 acc_train: 0.1929 loss_val: 1.9425 acc_val: 0.1567 time: 0.0338s\n",
      "Epoch: 0003 loss_train: 1.9334 acc_train: 0.2000 loss_val: 1.9315 acc_val: 0.1567 time: 0.0217s\n",
      "Epoch: 0004 loss_train: 1.9142 acc_train: 0.2000 loss_val: 1.9204 acc_val: 0.1567 time: 0.0226s\n",
      "Epoch: 0005 loss_train: 1.8960 acc_train: 0.2000 loss_val: 1.9098 acc_val: 0.1567 time: 0.0211s\n",
      "Epoch: 0006 loss_train: 1.8925 acc_train: 0.2000 loss_val: 1.8995 acc_val: 0.1567 time: 0.0217s\n",
      "Epoch: 0007 loss_train: 1.8818 acc_train: 0.2000 loss_val: 1.8897 acc_val: 0.1567 time: 0.0187s\n",
      "Epoch: 0008 loss_train: 1.8837 acc_train: 0.2000 loss_val: 1.8804 acc_val: 0.1567 time: 0.0199s\n",
      "Epoch: 0009 loss_train: 1.8660 acc_train: 0.2000 loss_val: 1.8714 acc_val: 0.1567 time: 0.0191s\n",
      "Epoch: 0010 loss_train: 1.8547 acc_train: 0.2000 loss_val: 1.8624 acc_val: 0.1567 time: 0.0190s\n",
      "Epoch: 0011 loss_train: 1.8580 acc_train: 0.2143 loss_val: 1.8532 acc_val: 0.1567 time: 0.0251s\n",
      "Epoch: 0012 loss_train: 1.8346 acc_train: 0.2429 loss_val: 1.8437 acc_val: 0.1567 time: 0.0242s\n",
      "Epoch: 0013 loss_train: 1.8278 acc_train: 0.2857 loss_val: 1.8337 acc_val: 0.1700 time: 0.0199s\n",
      "Epoch: 0014 loss_train: 1.8211 acc_train: 0.2500 loss_val: 1.8235 acc_val: 0.3167 time: 0.0193s\n",
      "Epoch: 0015 loss_train: 1.8077 acc_train: 0.3286 loss_val: 1.8135 acc_val: 0.4367 time: 0.0187s\n",
      "Epoch: 0016 loss_train: 1.7927 acc_train: 0.3786 loss_val: 1.8040 acc_val: 0.4533 time: 0.0181s\n",
      "Epoch: 0017 loss_train: 1.7879 acc_train: 0.4000 loss_val: 1.7947 acc_val: 0.4067 time: 0.0173s\n",
      "Epoch: 0018 loss_train: 1.7746 acc_train: 0.3857 loss_val: 1.7855 acc_val: 0.3767 time: 0.0202s\n",
      "Epoch: 0019 loss_train: 1.7831 acc_train: 0.3500 loss_val: 1.7764 acc_val: 0.3667 time: 0.0206s\n",
      "Epoch: 0020 loss_train: 1.7533 acc_train: 0.3143 loss_val: 1.7673 acc_val: 0.3567 time: 0.0233s\n",
      "Epoch: 0021 loss_train: 1.7416 acc_train: 0.3357 loss_val: 1.7583 acc_val: 0.3500 time: 0.0226s\n",
      "Epoch: 0022 loss_train: 1.7549 acc_train: 0.3357 loss_val: 1.7493 acc_val: 0.3500 time: 0.0183s\n",
      "Epoch: 0023 loss_train: 1.7271 acc_train: 0.3429 loss_val: 1.7406 acc_val: 0.3500 time: 0.0224s\n",
      "Epoch: 0024 loss_train: 1.7245 acc_train: 0.3286 loss_val: 1.7320 acc_val: 0.3500 time: 0.0225s\n",
      "Epoch: 0025 loss_train: 1.7256 acc_train: 0.3429 loss_val: 1.7238 acc_val: 0.3500 time: 0.0225s\n",
      "Epoch: 0026 loss_train: 1.6825 acc_train: 0.3000 loss_val: 1.7157 acc_val: 0.3500 time: 0.0197s\n",
      "Epoch: 0027 loss_train: 1.6911 acc_train: 0.3143 loss_val: 1.7079 acc_val: 0.3500 time: 0.0213s\n",
      "Epoch: 0028 loss_train: 1.6911 acc_train: 0.3429 loss_val: 1.7002 acc_val: 0.3500 time: 0.0218s\n",
      "Epoch: 0029 loss_train: 1.6736 acc_train: 0.3000 loss_val: 1.6925 acc_val: 0.3500 time: 0.0185s\n",
      "Epoch: 0030 loss_train: 1.6642 acc_train: 0.3500 loss_val: 1.6845 acc_val: 0.3500 time: 0.0219s\n",
      "Epoch: 0031 loss_train: 1.6702 acc_train: 0.3286 loss_val: 1.6764 acc_val: 0.3500 time: 0.0227s\n",
      "Epoch: 0032 loss_train: 1.6208 acc_train: 0.3643 loss_val: 1.6682 acc_val: 0.3500 time: 0.0190s\n",
      "Epoch: 0033 loss_train: 1.6084 acc_train: 0.3286 loss_val: 1.6596 acc_val: 0.3500 time: 0.0195s\n",
      "Epoch: 0034 loss_train: 1.6390 acc_train: 0.3214 loss_val: 1.6510 acc_val: 0.3533 time: 0.0215s\n",
      "Epoch: 0035 loss_train: 1.6360 acc_train: 0.3786 loss_val: 1.6423 acc_val: 0.3567 time: 0.0194s\n",
      "Epoch: 0036 loss_train: 1.5789 acc_train: 0.3500 loss_val: 1.6335 acc_val: 0.3600 time: 0.0220s\n",
      "Epoch: 0037 loss_train: 1.5848 acc_train: 0.3786 loss_val: 1.6246 acc_val: 0.3600 time: 0.0203s\n",
      "Epoch: 0038 loss_train: 1.5418 acc_train: 0.3714 loss_val: 1.6155 acc_val: 0.3600 time: 0.0216s\n",
      "Epoch: 0039 loss_train: 1.5415 acc_train: 0.4143 loss_val: 1.6064 acc_val: 0.3700 time: 0.0195s\n",
      "Epoch: 0040 loss_train: 1.5464 acc_train: 0.4643 loss_val: 1.5969 acc_val: 0.3867 time: 0.0218s\n",
      "Epoch: 0041 loss_train: 1.5321 acc_train: 0.4143 loss_val: 1.5873 acc_val: 0.4033 time: 0.0217s\n",
      "Epoch: 0042 loss_train: 1.4997 acc_train: 0.4214 loss_val: 1.5775 acc_val: 0.4267 time: 0.0217s\n",
      "Epoch: 0043 loss_train: 1.5012 acc_train: 0.4643 loss_val: 1.5676 acc_val: 0.4567 time: 0.0227s\n",
      "Epoch: 0044 loss_train: 1.4688 acc_train: 0.5286 loss_val: 1.5573 acc_val: 0.4800 time: 0.0209s\n",
      "Epoch: 0045 loss_train: 1.4712 acc_train: 0.5357 loss_val: 1.5467 acc_val: 0.4933 time: 0.0202s\n",
      "Epoch: 0046 loss_train: 1.4424 acc_train: 0.5500 loss_val: 1.5355 acc_val: 0.5033 time: 0.0209s\n",
      "Epoch: 0047 loss_train: 1.4369 acc_train: 0.5857 loss_val: 1.5236 acc_val: 0.5133 time: 0.0207s\n",
      "Epoch: 0048 loss_train: 1.4151 acc_train: 0.5786 loss_val: 1.5109 acc_val: 0.5200 time: 0.0233s\n",
      "Epoch: 0049 loss_train: 1.4220 acc_train: 0.5500 loss_val: 1.4977 acc_val: 0.5233 time: 0.0225s\n",
      "Epoch: 0050 loss_train: 1.3926 acc_train: 0.5429 loss_val: 1.4844 acc_val: 0.5267 time: 0.0230s\n",
      "Epoch: 0051 loss_train: 1.3766 acc_train: 0.6000 loss_val: 1.4714 acc_val: 0.5300 time: 0.0228s\n",
      "Epoch: 0052 loss_train: 1.3586 acc_train: 0.5571 loss_val: 1.4587 acc_val: 0.5367 time: 0.0303s\n",
      "Epoch: 0053 loss_train: 1.3283 acc_train: 0.6000 loss_val: 1.4458 acc_val: 0.5467 time: 0.0348s\n",
      "Epoch: 0054 loss_train: 1.3511 acc_train: 0.5643 loss_val: 1.4329 acc_val: 0.5467 time: 0.0348s\n",
      "Epoch: 0055 loss_train: 1.3123 acc_train: 0.5929 loss_val: 1.4204 acc_val: 0.5467 time: 0.0311s\n",
      "Epoch: 0056 loss_train: 1.3365 acc_train: 0.5857 loss_val: 1.4083 acc_val: 0.5567 time: 0.0324s\n",
      "Epoch: 0057 loss_train: 1.2964 acc_train: 0.6071 loss_val: 1.3961 acc_val: 0.5700 time: 0.0251s\n",
      "Epoch: 0058 loss_train: 1.2805 acc_train: 0.6000 loss_val: 1.3844 acc_val: 0.5833 time: 0.0267s\n",
      "Epoch: 0059 loss_train: 1.2463 acc_train: 0.6429 loss_val: 1.3726 acc_val: 0.5967 time: 0.0301s\n",
      "Epoch: 0060 loss_train: 1.2298 acc_train: 0.6786 loss_val: 1.3607 acc_val: 0.6033 time: 0.0274s\n",
      "Epoch: 0061 loss_train: 1.1961 acc_train: 0.6429 loss_val: 1.3488 acc_val: 0.6100 time: 0.0283s\n",
      "Epoch: 0062 loss_train: 1.1840 acc_train: 0.6714 loss_val: 1.3370 acc_val: 0.6067 time: 0.0284s\n",
      "Epoch: 0063 loss_train: 1.1915 acc_train: 0.6714 loss_val: 1.3249 acc_val: 0.6067 time: 0.0294s\n",
      "Epoch: 0064 loss_train: 1.1528 acc_train: 0.6929 loss_val: 1.3132 acc_val: 0.6133 time: 0.0295s\n",
      "Epoch: 0065 loss_train: 1.1612 acc_train: 0.6571 loss_val: 1.3018 acc_val: 0.6267 time: 0.0327s\n",
      "Epoch: 0066 loss_train: 1.1637 acc_train: 0.6643 loss_val: 1.2901 acc_val: 0.6467 time: 0.0357s\n",
      "Epoch: 0067 loss_train: 1.1534 acc_train: 0.6857 loss_val: 1.2789 acc_val: 0.6600 time: 0.0338s\n",
      "Epoch: 0068 loss_train: 1.1435 acc_train: 0.6714 loss_val: 1.2684 acc_val: 0.6567 time: 0.0249s\n",
      "Epoch: 0069 loss_train: 1.1080 acc_train: 0.6857 loss_val: 1.2577 acc_val: 0.6700 time: 0.0218s\n",
      "Epoch: 0070 loss_train: 1.0697 acc_train: 0.7357 loss_val: 1.2469 acc_val: 0.6833 time: 0.0187s\n",
      "Epoch: 0071 loss_train: 1.0882 acc_train: 0.6929 loss_val: 1.2357 acc_val: 0.6900 time: 0.0213s\n",
      "Epoch: 0072 loss_train: 1.0538 acc_train: 0.6857 loss_val: 1.2248 acc_val: 0.6967 time: 0.0215s\n",
      "Epoch: 0073 loss_train: 1.0472 acc_train: 0.7357 loss_val: 1.2142 acc_val: 0.7033 time: 0.0199s\n",
      "Epoch: 0074 loss_train: 1.0239 acc_train: 0.7643 loss_val: 1.2039 acc_val: 0.7067 time: 0.0251s\n",
      "Epoch: 0075 loss_train: 0.9917 acc_train: 0.7500 loss_val: 1.1942 acc_val: 0.7133 time: 0.0261s\n",
      "Epoch: 0076 loss_train: 1.0245 acc_train: 0.7571 loss_val: 1.1852 acc_val: 0.7200 time: 0.0301s\n",
      "Epoch: 0077 loss_train: 1.0124 acc_train: 0.7500 loss_val: 1.1762 acc_val: 0.7200 time: 0.0298s\n",
      "Epoch: 0078 loss_train: 1.0061 acc_train: 0.7929 loss_val: 1.1676 acc_val: 0.7200 time: 0.0301s\n",
      "Epoch: 0079 loss_train: 1.0054 acc_train: 0.7357 loss_val: 1.1592 acc_val: 0.7233 time: 0.0270s\n",
      "Epoch: 0080 loss_train: 0.9702 acc_train: 0.7786 loss_val: 1.1501 acc_val: 0.7233 time: 0.0299s\n",
      "Epoch: 0081 loss_train: 0.9392 acc_train: 0.7786 loss_val: 1.1408 acc_val: 0.7233 time: 0.0264s\n",
      "Epoch: 0082 loss_train: 0.9351 acc_train: 0.7643 loss_val: 1.1303 acc_val: 0.7200 time: 0.0305s\n",
      "Epoch: 0083 loss_train: 0.9631 acc_train: 0.8000 loss_val: 1.1192 acc_val: 0.7267 time: 0.0286s\n",
      "Epoch: 0084 loss_train: 0.9453 acc_train: 0.7429 loss_val: 1.1090 acc_val: 0.7267 time: 0.0270s\n",
      "Epoch: 0085 loss_train: 0.9227 acc_train: 0.7143 loss_val: 1.0998 acc_val: 0.7300 time: 0.0281s\n",
      "Epoch: 0086 loss_train: 0.9140 acc_train: 0.7714 loss_val: 1.0912 acc_val: 0.7367 time: 0.0266s\n",
      "Epoch: 0087 loss_train: 0.9201 acc_train: 0.7714 loss_val: 1.0827 acc_val: 0.7433 time: 0.0284s\n",
      "Epoch: 0088 loss_train: 0.9136 acc_train: 0.7714 loss_val: 1.0740 acc_val: 0.7467 time: 0.0271s\n",
      "Epoch: 0089 loss_train: 0.8705 acc_train: 0.8071 loss_val: 1.0652 acc_val: 0.7533 time: 0.0254s\n",
      "Epoch: 0090 loss_train: 0.8660 acc_train: 0.8000 loss_val: 1.0563 acc_val: 0.7533 time: 0.0250s\n",
      "Epoch: 0091 loss_train: 0.9001 acc_train: 0.7786 loss_val: 1.0471 acc_val: 0.7600 time: 0.0211s\n",
      "Epoch: 0092 loss_train: 0.8493 acc_train: 0.7786 loss_val: 1.0389 acc_val: 0.7633 time: 0.0232s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0093 loss_train: 0.8544 acc_train: 0.8000 loss_val: 1.0311 acc_val: 0.7700 time: 0.0187s\n",
      "Epoch: 0094 loss_train: 0.8037 acc_train: 0.8286 loss_val: 1.0233 acc_val: 0.7733 time: 0.0211s\n",
      "Epoch: 0095 loss_train: 0.8444 acc_train: 0.7929 loss_val: 1.0153 acc_val: 0.7700 time: 0.0273s\n",
      "Epoch: 0096 loss_train: 0.8794 acc_train: 0.7714 loss_val: 1.0082 acc_val: 0.7733 time: 0.0266s\n",
      "Epoch: 0097 loss_train: 0.8033 acc_train: 0.8500 loss_val: 1.0014 acc_val: 0.7733 time: 0.0253s\n",
      "Epoch: 0098 loss_train: 0.8183 acc_train: 0.7929 loss_val: 0.9938 acc_val: 0.7800 time: 0.0214s\n",
      "Epoch: 0099 loss_train: 0.7677 acc_train: 0.8500 loss_val: 0.9858 acc_val: 0.7800 time: 0.0234s\n",
      "Epoch: 0100 loss_train: 0.7993 acc_train: 0.8214 loss_val: 0.9789 acc_val: 0.7800 time: 0.0261s\n",
      "Epoch: 0101 loss_train: 0.7795 acc_train: 0.8286 loss_val: 0.9713 acc_val: 0.7800 time: 0.0283s\n",
      "Epoch: 0102 loss_train: 0.7801 acc_train: 0.8286 loss_val: 0.9639 acc_val: 0.7833 time: 0.0260s\n",
      "Epoch: 0103 loss_train: 0.7461 acc_train: 0.8500 loss_val: 0.9568 acc_val: 0.7833 time: 0.0252s\n",
      "Epoch: 0104 loss_train: 0.7501 acc_train: 0.8786 loss_val: 0.9505 acc_val: 0.7867 time: 0.0273s\n",
      "Epoch: 0105 loss_train: 0.7579 acc_train: 0.8357 loss_val: 0.9438 acc_val: 0.7867 time: 0.0254s\n",
      "Epoch: 0106 loss_train: 0.7266 acc_train: 0.8714 loss_val: 0.9365 acc_val: 0.7867 time: 0.0253s\n",
      "Epoch: 0107 loss_train: 0.6957 acc_train: 0.8500 loss_val: 0.9294 acc_val: 0.7933 time: 0.0260s\n",
      "Epoch: 0108 loss_train: 0.7291 acc_train: 0.8714 loss_val: 0.9226 acc_val: 0.7933 time: 0.0274s\n",
      "Epoch: 0109 loss_train: 0.7472 acc_train: 0.8286 loss_val: 0.9157 acc_val: 0.7933 time: 0.0275s\n",
      "Epoch: 0110 loss_train: 0.7020 acc_train: 0.8429 loss_val: 0.9095 acc_val: 0.7900 time: 0.0239s\n",
      "Epoch: 0111 loss_train: 0.6544 acc_train: 0.8857 loss_val: 0.9035 acc_val: 0.7900 time: 0.0235s\n",
      "Epoch: 0112 loss_train: 0.7139 acc_train: 0.8429 loss_val: 0.8983 acc_val: 0.7900 time: 0.0212s\n",
      "Epoch: 0113 loss_train: 0.6206 acc_train: 0.8571 loss_val: 0.8931 acc_val: 0.7967 time: 0.0166s\n",
      "Epoch: 0114 loss_train: 0.6843 acc_train: 0.8429 loss_val: 0.8880 acc_val: 0.8000 time: 0.0201s\n",
      "Epoch: 0115 loss_train: 0.6852 acc_train: 0.8500 loss_val: 0.8828 acc_val: 0.7967 time: 0.0204s\n",
      "Epoch: 0116 loss_train: 0.6853 acc_train: 0.8857 loss_val: 0.8781 acc_val: 0.8000 time: 0.0192s\n",
      "Epoch: 0117 loss_train: 0.6600 acc_train: 0.8714 loss_val: 0.8733 acc_val: 0.8033 time: 0.0207s\n",
      "Epoch: 0118 loss_train: 0.6390 acc_train: 0.8500 loss_val: 0.8683 acc_val: 0.8033 time: 0.0189s\n",
      "Epoch: 0119 loss_train: 0.6814 acc_train: 0.8500 loss_val: 0.8639 acc_val: 0.8033 time: 0.0216s\n",
      "Epoch: 0120 loss_train: 0.6250 acc_train: 0.8571 loss_val: 0.8605 acc_val: 0.8033 time: 0.0214s\n",
      "Epoch: 0121 loss_train: 0.6441 acc_train: 0.8786 loss_val: 0.8574 acc_val: 0.8033 time: 0.0209s\n",
      "Epoch: 0122 loss_train: 0.6034 acc_train: 0.8786 loss_val: 0.8534 acc_val: 0.8067 time: 0.0206s\n",
      "Epoch: 0123 loss_train: 0.5917 acc_train: 0.8714 loss_val: 0.8473 acc_val: 0.8067 time: 0.0184s\n",
      "Epoch: 0124 loss_train: 0.6402 acc_train: 0.8643 loss_val: 0.8416 acc_val: 0.8067 time: 0.0208s\n",
      "Epoch: 0125 loss_train: 0.5842 acc_train: 0.8786 loss_val: 0.8371 acc_val: 0.8067 time: 0.0191s\n",
      "Epoch: 0126 loss_train: 0.6056 acc_train: 0.8786 loss_val: 0.8325 acc_val: 0.8033 time: 0.0203s\n",
      "Epoch: 0127 loss_train: 0.6586 acc_train: 0.8357 loss_val: 0.8293 acc_val: 0.8000 time: 0.0202s\n",
      "Epoch: 0128 loss_train: 0.5907 acc_train: 0.9000 loss_val: 0.8264 acc_val: 0.8000 time: 0.0200s\n",
      "Epoch: 0129 loss_train: 0.6312 acc_train: 0.8500 loss_val: 0.8246 acc_val: 0.8033 time: 0.0227s\n",
      "Epoch: 0130 loss_train: 0.5674 acc_train: 0.9143 loss_val: 0.8222 acc_val: 0.8067 time: 0.0224s\n",
      "Epoch: 0131 loss_train: 0.5702 acc_train: 0.8857 loss_val: 0.8200 acc_val: 0.8067 time: 0.0208s\n",
      "Epoch: 0132 loss_train: 0.5915 acc_train: 0.9143 loss_val: 0.8162 acc_val: 0.8067 time: 0.0188s\n",
      "Epoch: 0133 loss_train: 0.5884 acc_train: 0.8857 loss_val: 0.8112 acc_val: 0.8100 time: 0.0195s\n",
      "Epoch: 0134 loss_train: 0.5518 acc_train: 0.9214 loss_val: 0.8059 acc_val: 0.8100 time: 0.0200s\n",
      "Epoch: 0135 loss_train: 0.5565 acc_train: 0.8929 loss_val: 0.8004 acc_val: 0.8100 time: 0.0186s\n",
      "Epoch: 0136 loss_train: 0.5226 acc_train: 0.9000 loss_val: 0.7956 acc_val: 0.8100 time: 0.0191s\n",
      "Epoch: 0137 loss_train: 0.5204 acc_train: 0.9143 loss_val: 0.7924 acc_val: 0.8067 time: 0.0210s\n",
      "Epoch: 0138 loss_train: 0.5369 acc_train: 0.9500 loss_val: 0.7898 acc_val: 0.8067 time: 0.0225s\n",
      "Epoch: 0139 loss_train: 0.5360 acc_train: 0.9214 loss_val: 0.7878 acc_val: 0.8067 time: 0.0243s\n",
      "Epoch: 0140 loss_train: 0.5547 acc_train: 0.9286 loss_val: 0.7851 acc_val: 0.8067 time: 0.0220s\n",
      "Epoch: 0141 loss_train: 0.5749 acc_train: 0.9000 loss_val: 0.7814 acc_val: 0.8067 time: 0.0204s\n",
      "Epoch: 0142 loss_train: 0.5201 acc_train: 0.9214 loss_val: 0.7774 acc_val: 0.8067 time: 0.0215s\n",
      "Epoch: 0143 loss_train: 0.5400 acc_train: 0.9214 loss_val: 0.7734 acc_val: 0.8100 time: 0.0221s\n",
      "Epoch: 0144 loss_train: 0.5286 acc_train: 0.9000 loss_val: 0.7693 acc_val: 0.8133 time: 0.0213s\n",
      "Epoch: 0145 loss_train: 0.5468 acc_train: 0.8929 loss_val: 0.7658 acc_val: 0.8167 time: 0.0212s\n",
      "Epoch: 0146 loss_train: 0.4876 acc_train: 0.9143 loss_val: 0.7630 acc_val: 0.8167 time: 0.0215s\n",
      "Epoch: 0147 loss_train: 0.5260 acc_train: 0.8929 loss_val: 0.7611 acc_val: 0.8167 time: 0.0193s\n",
      "Epoch: 0148 loss_train: 0.4767 acc_train: 0.9071 loss_val: 0.7598 acc_val: 0.8133 time: 0.0187s\n",
      "Epoch: 0149 loss_train: 0.5059 acc_train: 0.9214 loss_val: 0.7571 acc_val: 0.8133 time: 0.0226s\n",
      "Epoch: 0150 loss_train: 0.4725 acc_train: 0.9214 loss_val: 0.7547 acc_val: 0.8133 time: 0.0225s\n",
      "Epoch: 0151 loss_train: 0.4838 acc_train: 0.9143 loss_val: 0.7527 acc_val: 0.8133 time: 0.0203s\n",
      "Epoch: 0152 loss_train: 0.4823 acc_train: 0.9286 loss_val: 0.7497 acc_val: 0.8100 time: 0.0210s\n",
      "Epoch: 0153 loss_train: 0.5067 acc_train: 0.9071 loss_val: 0.7471 acc_val: 0.8100 time: 0.0213s\n",
      "Epoch: 0154 loss_train: 0.5058 acc_train: 0.9143 loss_val: 0.7460 acc_val: 0.8133 time: 0.0198s\n",
      "Epoch: 0155 loss_train: 0.4823 acc_train: 0.9214 loss_val: 0.7449 acc_val: 0.8133 time: 0.0200s\n",
      "Epoch: 0156 loss_train: 0.4751 acc_train: 0.9571 loss_val: 0.7437 acc_val: 0.8133 time: 0.0195s\n",
      "Epoch: 0157 loss_train: 0.4847 acc_train: 0.9286 loss_val: 0.7426 acc_val: 0.8167 time: 0.0190s\n",
      "Epoch: 0158 loss_train: 0.4708 acc_train: 0.8929 loss_val: 0.7410 acc_val: 0.8200 time: 0.0185s\n",
      "Epoch: 0159 loss_train: 0.4652 acc_train: 0.8857 loss_val: 0.7408 acc_val: 0.8233 time: 0.0205s\n",
      "Epoch: 0160 loss_train: 0.4808 acc_train: 0.8929 loss_val: 0.7397 acc_val: 0.8200 time: 0.0209s\n",
      "Epoch: 0161 loss_train: 0.4551 acc_train: 0.9143 loss_val: 0.7382 acc_val: 0.8200 time: 0.0177s\n",
      "Epoch: 0162 loss_train: 0.4553 acc_train: 0.8857 loss_val: 0.7358 acc_val: 0.8167 time: 0.0180s\n",
      "Epoch: 0163 loss_train: 0.4543 acc_train: 0.9000 loss_val: 0.7342 acc_val: 0.8100 time: 0.0172s\n",
      "Epoch: 0164 loss_train: 0.4540 acc_train: 0.9286 loss_val: 0.7335 acc_val: 0.8133 time: 0.0191s\n",
      "Epoch: 0165 loss_train: 0.4552 acc_train: 0.9357 loss_val: 0.7336 acc_val: 0.8167 time: 0.0184s\n",
      "Epoch: 0166 loss_train: 0.4543 acc_train: 0.9286 loss_val: 0.7327 acc_val: 0.8167 time: 0.0174s\n",
      "Epoch: 0167 loss_train: 0.4942 acc_train: 0.9143 loss_val: 0.7321 acc_val: 0.8100 time: 0.0180s\n",
      "Epoch: 0168 loss_train: 0.4624 acc_train: 0.9357 loss_val: 0.7296 acc_val: 0.8133 time: 0.0169s\n",
      "Epoch: 0169 loss_train: 0.4131 acc_train: 0.9357 loss_val: 0.7261 acc_val: 0.8167 time: 0.0185s\n",
      "Epoch: 0170 loss_train: 0.4629 acc_train: 0.9357 loss_val: 0.7226 acc_val: 0.8200 time: 0.0180s\n",
      "Epoch: 0171 loss_train: 0.4171 acc_train: 0.9429 loss_val: 0.7203 acc_val: 0.8167 time: 0.0187s\n",
      "Epoch: 0172 loss_train: 0.4513 acc_train: 0.9429 loss_val: 0.7175 acc_val: 0.8133 time: 0.0255s\n",
      "Epoch: 0173 loss_train: 0.4032 acc_train: 0.9214 loss_val: 0.7153 acc_val: 0.8100 time: 0.0222s\n",
      "Epoch: 0174 loss_train: 0.4462 acc_train: 0.9357 loss_val: 0.7133 acc_val: 0.8167 time: 0.0164s\n",
      "Epoch: 0175 loss_train: 0.4541 acc_train: 0.9143 loss_val: 0.7134 acc_val: 0.8100 time: 0.0149s\n",
      "Epoch: 0176 loss_train: 0.4179 acc_train: 0.9214 loss_val: 0.7162 acc_val: 0.8167 time: 0.0147s\n",
      "Epoch: 0177 loss_train: 0.4597 acc_train: 0.9214 loss_val: 0.7183 acc_val: 0.8167 time: 0.0159s\n",
      "Epoch: 0178 loss_train: 0.4308 acc_train: 0.9286 loss_val: 0.7180 acc_val: 0.8133 time: 0.0187s\n",
      "Epoch: 0179 loss_train: 0.4348 acc_train: 0.9286 loss_val: 0.7158 acc_val: 0.8133 time: 0.0158s\n",
      "Epoch: 0180 loss_train: 0.4207 acc_train: 0.9357 loss_val: 0.7137 acc_val: 0.8100 time: 0.0159s\n",
      "Epoch: 0181 loss_train: 0.4082 acc_train: 0.9571 loss_val: 0.7106 acc_val: 0.8133 time: 0.0161s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0182 loss_train: 0.4314 acc_train: 0.9214 loss_val: 0.7082 acc_val: 0.8067 time: 0.0182s\n",
      "Epoch: 0183 loss_train: 0.4213 acc_train: 0.9571 loss_val: 0.7062 acc_val: 0.8100 time: 0.0189s\n",
      "Epoch: 0184 loss_train: 0.3942 acc_train: 0.9357 loss_val: 0.7046 acc_val: 0.8100 time: 0.0186s\n",
      "Epoch: 0185 loss_train: 0.3907 acc_train: 0.9429 loss_val: 0.7034 acc_val: 0.8100 time: 0.0193s\n",
      "Epoch: 0186 loss_train: 0.4236 acc_train: 0.9286 loss_val: 0.7035 acc_val: 0.8100 time: 0.0173s\n",
      "Epoch: 0187 loss_train: 0.4514 acc_train: 0.9143 loss_val: 0.7049 acc_val: 0.8033 time: 0.0160s\n",
      "Epoch: 0188 loss_train: 0.4297 acc_train: 0.9143 loss_val: 0.7047 acc_val: 0.8100 time: 0.0204s\n",
      "Epoch: 0189 loss_train: 0.3762 acc_train: 0.9357 loss_val: 0.7040 acc_val: 0.8100 time: 0.0164s\n",
      "Epoch: 0190 loss_train: 0.3895 acc_train: 0.9286 loss_val: 0.7051 acc_val: 0.8067 time: 0.0181s\n",
      "Epoch: 0191 loss_train: 0.4237 acc_train: 0.9571 loss_val: 0.7047 acc_val: 0.8067 time: 0.0178s\n",
      "Epoch: 0192 loss_train: 0.4496 acc_train: 0.9357 loss_val: 0.7031 acc_val: 0.8067 time: 0.0188s\n",
      "Epoch: 0193 loss_train: 0.3820 acc_train: 0.9286 loss_val: 0.7008 acc_val: 0.8067 time: 0.0186s\n",
      "Epoch: 0194 loss_train: 0.3940 acc_train: 0.9500 loss_val: 0.6985 acc_val: 0.8067 time: 0.0180s\n",
      "Epoch: 0195 loss_train: 0.4104 acc_train: 0.9571 loss_val: 0.6944 acc_val: 0.8100 time: 0.0213s\n",
      "Epoch: 0196 loss_train: 0.3978 acc_train: 0.9357 loss_val: 0.6911 acc_val: 0.8100 time: 0.0181s\n",
      "Epoch: 0197 loss_train: 0.3740 acc_train: 0.9714 loss_val: 0.6881 acc_val: 0.8067 time: 0.0186s\n",
      "Epoch: 0198 loss_train: 0.4091 acc_train: 0.9429 loss_val: 0.6870 acc_val: 0.8100 time: 0.0172s\n",
      "Epoch: 0199 loss_train: 0.3835 acc_train: 0.9357 loss_val: 0.6886 acc_val: 0.8100 time: 0.0192s\n",
      "Epoch: 0200 loss_train: 0.3634 acc_train: 0.9429 loss_val: 0.6915 acc_val: 0.8033 time: 0.0180s\n",
      "Optimization Finished!\n",
      "Total time elapsed: 4.8413s\n",
      "Test set results: loss= 0.7168 accuracy= 0.8370\n"
     ]
    }
   ],
   "source": [
    "# train.py\n",
    "\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "# Training settings\n",
    "no_cuda = False\n",
    "fastmode = False\n",
    "seed = 42\n",
    "epochs = 200\n",
    "lr = 0.01\n",
    "weight_decay = 5e-4\n",
    "hidden = 16\n",
    "dropout = 0.5\n",
    "\n",
    "cuda = not no_cuda and torch.cuda.is_available()\n",
    "\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if cuda:\n",
    "    torch.cuda.manual_seed(seed)\n",
    "\n",
    "# Load data\n",
    "adj, features, labels, idx_train, idx_val, idx_test = load_data()\n",
    "\n",
    "# Model and optimizer\n",
    "model = GCN(nfeat=features.shape[1],\n",
    "            nhid=hidden,\n",
    "            nclass=labels.max().item() + 1,\n",
    "            dropout=dropout)\n",
    "optimizer = optim.Adam(model.parameters(),\n",
    "                       lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "if cuda:\n",
    "    model.cuda()\n",
    "    features = features.cuda()\n",
    "    adj = adj.cuda()\n",
    "    labels = labels.cuda()\n",
    "    idx_train = idx_train.cuda()\n",
    "    idx_val = idx_val.cuda()\n",
    "    idx_test = idx_test.cuda()\n",
    "\n",
    "\n",
    "def train(epoch):\n",
    "    t = time.time()\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(features, adj)\n",
    "    loss_train = F.nll_loss(output[idx_train], labels[idx_train])\n",
    "    acc_train = accuracy(output[idx_train], labels[idx_train])\n",
    "    loss_train.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if not fastmode:\n",
    "        # Evaluate validation set performance separately,\n",
    "        # deactivates dropout during validation run.\n",
    "        model.eval()\n",
    "        output = model(features, adj)\n",
    "\n",
    "    loss_val = F.nll_loss(output[idx_val], labels[idx_val])\n",
    "    acc_val = accuracy(output[idx_val], labels[idx_val])\n",
    "    print('Epoch: {:04d}'.format(epoch+1),\n",
    "          'loss_train: {:.4f}'.format(loss_train.item()),\n",
    "          'acc_train: {:.4f}'.format(acc_train.item()),\n",
    "          'loss_val: {:.4f}'.format(loss_val.item()),\n",
    "          'acc_val: {:.4f}'.format(acc_val.item()),\n",
    "          'time: {:.4f}s'.format(time.time() - t))\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    output = model(features, adj)\n",
    "    loss_test = F.nll_loss(output[idx_test], labels[idx_test])\n",
    "    acc_test = accuracy(output[idx_test], labels[idx_test])\n",
    "    print(\"Test set results:\",\n",
    "          \"loss= {:.4f}\".format(loss_test.item()),\n",
    "          \"accuracy= {:.4f}\".format(acc_test.item()))\n",
    "\n",
    "# Train model\n",
    "t_total = time.time()\n",
    "for epoch in range(epochs):\n",
    "    train(epoch)\n",
    "print(\"Optimization Finished!\")\n",
    "print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))\n",
    "\n",
    "# Testing\n",
    "test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
