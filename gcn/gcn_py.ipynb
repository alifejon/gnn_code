{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<style>.container { width:100% !important; }</style>",
      "text/plain": "<IPython.core.display.HTML object>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Graph Convolutional Networks in PyTorch\n",
    "https://github.com/tkipf/pygcn\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils.py\n",
    "\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import torch\n",
    "import math\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.nn.modules.module import Module\n",
    "\n",
    "\n",
    "def encode_onehot(labels):\n",
    "    classes = set(labels)\n",
    "    classes_dict = {c: np.identity(len(classes))[i, :] for i, c in\n",
    "                    enumerate(classes)}\n",
    "    labels_onehot = np.array(list(map(classes_dict.get, labels)),\n",
    "                             dtype=np.int32)\n",
    "    return labels_onehot\n",
    "\n",
    "\n",
    "def load_data(path=\"data/cora/\", dataset=\"cora\"):\n",
    "    \"\"\"Load citation network dataset (cora only for now)\"\"\"\n",
    "    print('Loading {} dataset...'.format(dataset))\n",
    "\n",
    "    idx_features_labels = np.genfromtxt(\"{}{}.content\".format(path, dataset),\n",
    "                                        dtype=np.dtype(str))\n",
    "    features = sp.csr_matrix(idx_features_labels[:, 1:-1], dtype=np.float32)\n",
    "    labels = encode_onehot(idx_features_labels[:, -1])\n",
    "\n",
    "    # build graph\n",
    "    idx = np.array(idx_features_labels[:, 0], dtype=np.int32)\n",
    "    idx_map = {j: i for i, j in enumerate(idx)}\n",
    "    edges_unordered = np.genfromtxt(\"{}{}.cites\".format(path, dataset),\n",
    "                                    dtype=np.int32)\n",
    "    edges = np.array(list(map(idx_map.get, edges_unordered.flatten())),\n",
    "                     dtype=np.int32).reshape(edges_unordered.shape)\n",
    "    adj = sp.coo_matrix((np.ones(edges.shape[0]), (edges[:, 0], edges[:, 1])),\n",
    "                        shape=(labels.shape[0], labels.shape[0]),\n",
    "                        dtype=np.float32)\n",
    "\n",
    "    # build symmetric adjacency matrix\n",
    "    adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n",
    "\n",
    "    features = normalize(features)\n",
    "    adj = normalize(adj + sp.eye(adj.shape[0]))\n",
    "\n",
    "    idx_train = range(140)\n",
    "    idx_val = range(200, 500)\n",
    "    idx_test = range(500, 1500)\n",
    "\n",
    "    features = torch.FloatTensor(np.array(features.todense()))\n",
    "    labels = torch.LongTensor(np.where(labels)[1])\n",
    "    adj = sparse_mx_to_torch_sparse_tensor(adj)\n",
    "\n",
    "    idx_train = torch.LongTensor(idx_train)\n",
    "    idx_val = torch.LongTensor(idx_val)\n",
    "    idx_test = torch.LongTensor(idx_test)\n",
    "\n",
    "    return adj, features, labels, idx_train, idx_val, idx_test\n",
    "\n",
    "\n",
    "def normalize(mx):\n",
    "    \"\"\"Row-normalize sparse matrix\"\"\"\n",
    "    rowsum = np.array(mx.sum(1))\n",
    "    r_inv = np.power(rowsum, -1).flatten()\n",
    "    r_inv[np.isinf(r_inv)] = 0.\n",
    "    r_mat_inv = sp.diags(r_inv)\n",
    "    mx = r_mat_inv.dot(mx)\n",
    "    return mx\n",
    "\n",
    "\n",
    "def accuracy(output, labels):\n",
    "    preds = output.max(1)[1].type_as(labels)\n",
    "    correct = preds.eq(labels).double()\n",
    "    correct = correct.sum()\n",
    "    return correct / len(labels)\n",
    "\n",
    "\n",
    "def sparse_mx_to_torch_sparse_tensor(sparse_mx):\n",
    "    \"\"\"Convert a scipy sparse matrix to a torch sparse tensor.\"\"\"\n",
    "    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
    "    indices = torch.from_numpy(\n",
    "        np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n",
    "    values = torch.from_numpy(sparse_mx.data)\n",
    "    shape = torch.Size(sparse_mx.shape)\n",
    "    return torch.sparse.FloatTensor(indices, values, shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "\n",
    "\n",
    "class GraphConvolution(Module):\n",
    "    \"\"\"\n",
    "    Simple GCN layer, similar to https://arxiv.org/abs/1609.02907\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super(GraphConvolution, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = Parameter(torch.FloatTensor(in_features, out_features))\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.FloatTensor(out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, input, adj):\n",
    "        support = torch.mm(input, self.weight)\n",
    "        output = torch.spmm(adj, support)\n",
    "        if self.bias is not None:\n",
    "            return output + self.bias\n",
    "        else:\n",
    "            return output\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + ' (' \\\n",
    "               + str(self.in_features) + ' -> ' \\\n",
    "               + str(self.out_features) + ')'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models.py\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, nfeat, nhid, nclass, dropout):\n",
    "        super(GCN, self).__init__()\n",
    "\n",
    "        self.gc1 = GraphConvolution(nfeat, nhid)\n",
    "        self.gc2 = GraphConvolution(nhid, nclass)\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        x = F.relu(self.gc1(x, adj))\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        x = self.gc2(x, adj)\n",
    "        return F.log_softmax(x, dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cora dataset...\n",
      "Epoch: 0001 loss_train: 1.9633 acc_train: 0.1500 loss_val: 1.9638 acc_val: 0.1133 time: 0.0742s\n",
      "Epoch: 0002 loss_train: 1.9601 acc_train: 0.1286 loss_val: 1.9490 acc_val: 0.1200 time: 0.0130s\n",
      "Epoch: 0003 loss_train: 1.9410 acc_train: 0.1357 loss_val: 1.9345 acc_val: 0.1233 time: 0.0150s\n",
      "Epoch: 0004 loss_train: 1.9285 acc_train: 0.1929 loss_val: 1.9201 acc_val: 0.1500 time: 0.0150s\n",
      "Epoch: 0005 loss_train: 1.9101 acc_train: 0.1571 loss_val: 1.9060 acc_val: 0.1900 time: 0.0110s\n",
      "Epoch: 0006 loss_train: 1.8966 acc_train: 0.2714 loss_val: 1.8922 acc_val: 0.3467 time: 0.0120s\n",
      "Epoch: 0007 loss_train: 1.8828 acc_train: 0.2857 loss_val: 1.8789 acc_val: 0.3467 time: 0.0120s\n",
      "Epoch: 0008 loss_train: 1.8753 acc_train: 0.2929 loss_val: 1.8660 acc_val: 0.3500 time: 0.0110s\n",
      "Epoch: 0009 loss_train: 1.8611 acc_train: 0.3500 loss_val: 1.8537 acc_val: 0.3533 time: 0.0100s\n",
      "Epoch: 0010 loss_train: 1.8425 acc_train: 0.3071 loss_val: 1.8418 acc_val: 0.3600 time: 0.0130s\n",
      "Epoch: 0011 loss_train: 1.8484 acc_train: 0.3286 loss_val: 1.8308 acc_val: 0.3700 time: 0.0130s\n",
      "Epoch: 0012 loss_train: 1.8139 acc_train: 0.3429 loss_val: 1.8205 acc_val: 0.3767 time: 0.0100s\n",
      "Epoch: 0013 loss_train: 1.8044 acc_train: 0.3714 loss_val: 1.8109 acc_val: 0.3933 time: 0.0120s\n",
      "Epoch: 0014 loss_train: 1.7995 acc_train: 0.3929 loss_val: 1.8020 acc_val: 0.3967 time: 0.0130s\n",
      "Epoch: 0015 loss_train: 1.7909 acc_train: 0.4143 loss_val: 1.7937 acc_val: 0.4167 time: 0.0100s\n",
      "Epoch: 0016 loss_train: 1.7689 acc_train: 0.3714 loss_val: 1.7860 acc_val: 0.4200 time: 0.0100s\n",
      "Epoch: 0017 loss_train: 1.7659 acc_train: 0.3929 loss_val: 1.7787 acc_val: 0.4067 time: 0.0100s\n",
      "Epoch: 0018 loss_train: 1.7462 acc_train: 0.4000 loss_val: 1.7715 acc_val: 0.3900 time: 0.0110s\n",
      "Epoch: 0019 loss_train: 1.7659 acc_train: 0.3643 loss_val: 1.7644 acc_val: 0.3800 time: 0.0100s\n",
      "Epoch: 0020 loss_train: 1.7220 acc_train: 0.3929 loss_val: 1.7571 acc_val: 0.3700 time: 0.0100s\n",
      "Epoch: 0021 loss_train: 1.7289 acc_train: 0.4143 loss_val: 1.7499 acc_val: 0.3667 time: 0.0110s\n",
      "Epoch: 0022 loss_train: 1.7457 acc_train: 0.3500 loss_val: 1.7424 acc_val: 0.3600 time: 0.0100s\n",
      "Epoch: 0023 loss_train: 1.7094 acc_train: 0.3714 loss_val: 1.7347 acc_val: 0.3567 time: 0.0100s\n",
      "Epoch: 0024 loss_train: 1.7130 acc_train: 0.3643 loss_val: 1.7269 acc_val: 0.3567 time: 0.0120s\n",
      "Epoch: 0025 loss_train: 1.7183 acc_train: 0.3143 loss_val: 1.7194 acc_val: 0.3533 time: 0.0110s\n",
      "Epoch: 0026 loss_train: 1.6795 acc_train: 0.3286 loss_val: 1.7119 acc_val: 0.3500 time: 0.0110s\n",
      "Epoch: 0027 loss_train: 1.6628 acc_train: 0.3214 loss_val: 1.7042 acc_val: 0.3500 time: 0.0100s\n",
      "Epoch: 0028 loss_train: 1.6801 acc_train: 0.3143 loss_val: 1.6964 acc_val: 0.3500 time: 0.0100s\n",
      "Epoch: 0029 loss_train: 1.6589 acc_train: 0.3214 loss_val: 1.6883 acc_val: 0.3500 time: 0.0110s\n",
      "Epoch: 0030 loss_train: 1.6676 acc_train: 0.3643 loss_val: 1.6800 acc_val: 0.3500 time: 0.0100s\n",
      "Epoch: 0031 loss_train: 1.6544 acc_train: 0.3357 loss_val: 1.6715 acc_val: 0.3533 time: 0.0090s\n",
      "Epoch: 0032 loss_train: 1.6236 acc_train: 0.3714 loss_val: 1.6627 acc_val: 0.3533 time: 0.0090s\n",
      "Epoch: 0033 loss_train: 1.5992 acc_train: 0.3714 loss_val: 1.6540 acc_val: 0.3600 time: 0.0100s\n",
      "Epoch: 0034 loss_train: 1.6013 acc_train: 0.3500 loss_val: 1.6452 acc_val: 0.3600 time: 0.0090s\n",
      "Epoch: 0035 loss_train: 1.6026 acc_train: 0.4000 loss_val: 1.6362 acc_val: 0.3633 time: 0.0100s\n",
      "Epoch: 0036 loss_train: 1.5634 acc_train: 0.4214 loss_val: 1.6271 acc_val: 0.3767 time: 0.0100s\n",
      "Epoch: 0037 loss_train: 1.5664 acc_train: 0.4143 loss_val: 1.6178 acc_val: 0.3900 time: 0.0100s\n",
      "Epoch: 0038 loss_train: 1.5385 acc_train: 0.4000 loss_val: 1.6083 acc_val: 0.3967 time: 0.0110s\n",
      "Epoch: 0039 loss_train: 1.5397 acc_train: 0.4214 loss_val: 1.5988 acc_val: 0.4333 time: 0.0110s\n",
      "Epoch: 0040 loss_train: 1.5073 acc_train: 0.5000 loss_val: 1.5885 acc_val: 0.4433 time: 0.0100s\n",
      "Epoch: 0041 loss_train: 1.5104 acc_train: 0.4714 loss_val: 1.5781 acc_val: 0.4533 time: 0.0110s\n",
      "Epoch: 0042 loss_train: 1.4818 acc_train: 0.4714 loss_val: 1.5676 acc_val: 0.4667 time: 0.0100s\n",
      "Epoch: 0043 loss_train: 1.4762 acc_train: 0.5000 loss_val: 1.5567 acc_val: 0.4933 time: 0.0100s\n",
      "Epoch: 0044 loss_train: 1.4727 acc_train: 0.5429 loss_val: 1.5450 acc_val: 0.4933 time: 0.0110s\n",
      "Epoch: 0045 loss_train: 1.4557 acc_train: 0.5571 loss_val: 1.5325 acc_val: 0.4967 time: 0.0110s\n",
      "Epoch: 0046 loss_train: 1.4340 acc_train: 0.5571 loss_val: 1.5196 acc_val: 0.5067 time: 0.0110s\n",
      "Epoch: 0047 loss_train: 1.4145 acc_train: 0.5643 loss_val: 1.5067 acc_val: 0.5200 time: 0.0090s\n",
      "Epoch: 0048 loss_train: 1.4127 acc_train: 0.5429 loss_val: 1.4938 acc_val: 0.5233 time: 0.0110s\n",
      "Epoch: 0049 loss_train: 1.4008 acc_train: 0.5857 loss_val: 1.4808 acc_val: 0.5233 time: 0.0110s\n",
      "Epoch: 0050 loss_train: 1.3678 acc_train: 0.5714 loss_val: 1.4679 acc_val: 0.5367 time: 0.0120s\n",
      "Epoch: 0051 loss_train: 1.3417 acc_train: 0.5929 loss_val: 1.4551 acc_val: 0.5567 time: 0.0120s\n",
      "Epoch: 0052 loss_train: 1.3416 acc_train: 0.5714 loss_val: 1.4422 acc_val: 0.5700 time: 0.0110s\n",
      "Epoch: 0053 loss_train: 1.2852 acc_train: 0.6857 loss_val: 1.4293 acc_val: 0.5867 time: 0.0120s\n",
      "Epoch: 0054 loss_train: 1.3317 acc_train: 0.6071 loss_val: 1.4164 acc_val: 0.6100 time: 0.0110s\n",
      "Epoch: 0055 loss_train: 1.2783 acc_train: 0.6571 loss_val: 1.4036 acc_val: 0.6200 time: 0.0110s\n",
      "Epoch: 0056 loss_train: 1.2819 acc_train: 0.6500 loss_val: 1.3909 acc_val: 0.6367 time: 0.0120s\n",
      "Epoch: 0057 loss_train: 1.2890 acc_train: 0.6929 loss_val: 1.3781 acc_val: 0.6567 time: 0.0090s\n",
      "Epoch: 0058 loss_train: 1.2343 acc_train: 0.6857 loss_val: 1.3656 acc_val: 0.6667 time: 0.0097s\n",
      "Epoch: 0059 loss_train: 1.2301 acc_train: 0.6857 loss_val: 1.3524 acc_val: 0.6800 time: 0.0100s\n",
      "Epoch: 0060 loss_train: 1.2151 acc_train: 0.7643 loss_val: 1.3386 acc_val: 0.6833 time: 0.0110s\n",
      "Epoch: 0061 loss_train: 1.1767 acc_train: 0.7357 loss_val: 1.3248 acc_val: 0.6867 time: 0.0110s\n",
      "Epoch: 0062 loss_train: 1.1320 acc_train: 0.7143 loss_val: 1.3113 acc_val: 0.6867 time: 0.0110s\n",
      "Epoch: 0063 loss_train: 1.1874 acc_train: 0.6714 loss_val: 1.2984 acc_val: 0.6867 time: 0.0120s\n",
      "Epoch: 0064 loss_train: 1.1362 acc_train: 0.7714 loss_val: 1.2853 acc_val: 0.6900 time: 0.0110s\n",
      "Epoch: 0065 loss_train: 1.1135 acc_train: 0.7714 loss_val: 1.2727 acc_val: 0.6900 time: 0.0100s\n",
      "Epoch: 0066 loss_train: 1.0977 acc_train: 0.7857 loss_val: 1.2602 acc_val: 0.6900 time: 0.0110s\n",
      "Epoch: 0067 loss_train: 1.1133 acc_train: 0.7500 loss_val: 1.2485 acc_val: 0.7033 time: 0.0110s\n",
      "Epoch: 0068 loss_train: 1.0729 acc_train: 0.7786 loss_val: 1.2375 acc_val: 0.7267 time: 0.0100s\n",
      "Epoch: 0069 loss_train: 1.0596 acc_train: 0.7571 loss_val: 1.2261 acc_val: 0.7333 time: 0.0100s\n",
      "Epoch: 0070 loss_train: 1.0575 acc_train: 0.8000 loss_val: 1.2150 acc_val: 0.7367 time: 0.0100s\n",
      "Epoch: 0071 loss_train: 1.0353 acc_train: 0.7857 loss_val: 1.2041 acc_val: 0.7400 time: 0.0110s\n",
      "Epoch: 0072 loss_train: 1.0147 acc_train: 0.8000 loss_val: 1.1936 acc_val: 0.7533 time: 0.0110s\n",
      "Epoch: 0073 loss_train: 0.9965 acc_train: 0.8000 loss_val: 1.1829 acc_val: 0.7567 time: 0.0100s\n",
      "Epoch: 0074 loss_train: 1.0002 acc_train: 0.8214 loss_val: 1.1723 acc_val: 0.7633 time: 0.0100s\n",
      "Epoch: 0075 loss_train: 0.9518 acc_train: 0.8286 loss_val: 1.1618 acc_val: 0.7667 time: 0.0110s\n",
      "Epoch: 0076 loss_train: 0.9517 acc_train: 0.8429 loss_val: 1.1513 acc_val: 0.7733 time: 0.0120s\n",
      "Epoch: 0077 loss_train: 0.9411 acc_train: 0.8357 loss_val: 1.1393 acc_val: 0.7733 time: 0.0110s\n",
      "Epoch: 0078 loss_train: 0.9685 acc_train: 0.8286 loss_val: 1.1277 acc_val: 0.7733 time: 0.0110s\n",
      "Epoch: 0079 loss_train: 0.9489 acc_train: 0.7929 loss_val: 1.1170 acc_val: 0.7767 time: 0.0110s\n",
      "Epoch: 0080 loss_train: 0.9249 acc_train: 0.8286 loss_val: 1.1060 acc_val: 0.7733 time: 0.0100s\n",
      "Epoch: 0081 loss_train: 0.8530 acc_train: 0.8429 loss_val: 1.0949 acc_val: 0.7733 time: 0.0110s\n",
      "Epoch: 0082 loss_train: 0.8911 acc_train: 0.8429 loss_val: 1.0841 acc_val: 0.7800 time: 0.0120s\n",
      "Epoch: 0083 loss_train: 0.9128 acc_train: 0.8286 loss_val: 1.0742 acc_val: 0.7800 time: 0.0100s\n",
      "Epoch: 0084 loss_train: 0.8658 acc_train: 0.8429 loss_val: 1.0653 acc_val: 0.7867 time: 0.0110s\n",
      "Epoch: 0085 loss_train: 0.8524 acc_train: 0.8071 loss_val: 1.0576 acc_val: 0.7933 time: 0.0110s\n",
      "Epoch: 0086 loss_train: 0.8434 acc_train: 0.8714 loss_val: 1.0509 acc_val: 0.7967 time: 0.0090s\n",
      "Epoch: 0087 loss_train: 0.8431 acc_train: 0.8214 loss_val: 1.0440 acc_val: 0.7867 time: 0.0110s\n",
      "Epoch: 0088 loss_train: 0.8650 acc_train: 0.8000 loss_val: 1.0357 acc_val: 0.7867 time: 0.0120s\n",
      "Epoch: 0089 loss_train: 0.8204 acc_train: 0.8357 loss_val: 1.0257 acc_val: 0.7933 time: 0.0120s\n",
      "Epoch: 0090 loss_train: 0.8168 acc_train: 0.8357 loss_val: 1.0152 acc_val: 0.7933 time: 0.0100s\n",
      "Epoch: 0091 loss_train: 0.8405 acc_train: 0.8286 loss_val: 1.0042 acc_val: 0.7933 time: 0.0110s\n",
      "Epoch: 0092 loss_train: 0.7647 acc_train: 0.8786 loss_val: 0.9951 acc_val: 0.8000 time: 0.0100s\n",
      "Epoch: 0093 loss_train: 0.7739 acc_train: 0.8643 loss_val: 0.9866 acc_val: 0.8033 time: 0.0120s\n",
      "Epoch: 0094 loss_train: 0.7500 acc_train: 0.8857 loss_val: 0.9786 acc_val: 0.8033 time: 0.0110s\n",
      "Epoch: 0095 loss_train: 0.7977 acc_train: 0.8143 loss_val: 0.9715 acc_val: 0.8000 time: 0.0110s\n",
      "Epoch: 0096 loss_train: 0.7626 acc_train: 0.8286 loss_val: 0.9652 acc_val: 0.8000 time: 0.0100s\n",
      "Epoch: 0097 loss_train: 0.7671 acc_train: 0.8429 loss_val: 0.9596 acc_val: 0.7967 time: 0.0100s\n",
      "Epoch: 0098 loss_train: 0.7179 acc_train: 0.8357 loss_val: 0.9539 acc_val: 0.7967 time: 0.0110s\n",
      "Epoch: 0099 loss_train: 0.7372 acc_train: 0.8571 loss_val: 0.9484 acc_val: 0.7833 time: 0.0100s\n",
      "Epoch: 0100 loss_train: 0.7800 acc_train: 0.8000 loss_val: 0.9441 acc_val: 0.7867 time: 0.0110s\n",
      "Epoch: 0101 loss_train: 0.7053 acc_train: 0.8857 loss_val: 0.9401 acc_val: 0.7900 time: 0.0110s\n",
      "Epoch: 0102 loss_train: 0.7050 acc_train: 0.8643 loss_val: 0.9364 acc_val: 0.7867 time: 0.0100s\n",
      "Epoch: 0103 loss_train: 0.7139 acc_train: 0.8429 loss_val: 0.9318 acc_val: 0.7867 time: 0.0100s\n",
      "Epoch: 0104 loss_train: 0.7048 acc_train: 0.8714 loss_val: 0.9264 acc_val: 0.7867 time: 0.0100s\n",
      "Epoch: 0105 loss_train: 0.7094 acc_train: 0.8929 loss_val: 0.9189 acc_val: 0.7900 time: 0.0110s\n",
      "Epoch: 0106 loss_train: 0.7235 acc_train: 0.8857 loss_val: 0.9109 acc_val: 0.7900 time: 0.0110s\n",
      "Epoch: 0107 loss_train: 0.6677 acc_train: 0.9000 loss_val: 0.9040 acc_val: 0.7900 time: 0.0100s\n",
      "Epoch: 0108 loss_train: 0.6914 acc_train: 0.8286 loss_val: 0.8977 acc_val: 0.8000 time: 0.0110s\n",
      "Epoch: 0109 loss_train: 0.6624 acc_train: 0.8929 loss_val: 0.8922 acc_val: 0.8067 time: 0.0110s\n",
      "Epoch: 0110 loss_train: 0.6586 acc_train: 0.8857 loss_val: 0.8875 acc_val: 0.8100 time: 0.0100s\n",
      "Epoch: 0111 loss_train: 0.6547 acc_train: 0.9071 loss_val: 0.8831 acc_val: 0.8100 time: 0.0110s\n",
      "Epoch: 0112 loss_train: 0.6259 acc_train: 0.8929 loss_val: 0.8791 acc_val: 0.8067 time: 0.0110s\n",
      "Epoch: 0113 loss_train: 0.6223 acc_train: 0.8714 loss_val: 0.8759 acc_val: 0.7933 time: 0.0110s\n",
      "Epoch: 0114 loss_train: 0.6188 acc_train: 0.8571 loss_val: 0.8732 acc_val: 0.7933 time: 0.0100s\n",
      "Epoch: 0115 loss_train: 0.6275 acc_train: 0.8929 loss_val: 0.8703 acc_val: 0.7900 time: 0.0100s\n",
      "Epoch: 0116 loss_train: 0.6705 acc_train: 0.8714 loss_val: 0.8672 acc_val: 0.7900 time: 0.0100s\n",
      "Epoch: 0117 loss_train: 0.5949 acc_train: 0.9071 loss_val: 0.8639 acc_val: 0.7900 time: 0.0100s\n",
      "Epoch: 0118 loss_train: 0.6022 acc_train: 0.8786 loss_val: 0.8594 acc_val: 0.7933 time: 0.0110s\n",
      "Epoch: 0119 loss_train: 0.6245 acc_train: 0.8643 loss_val: 0.8553 acc_val: 0.7933 time: 0.0100s\n",
      "Epoch: 0120 loss_train: 0.6072 acc_train: 0.8643 loss_val: 0.8515 acc_val: 0.7933 time: 0.0100s\n",
      "Epoch: 0121 loss_train: 0.6339 acc_train: 0.8714 loss_val: 0.8475 acc_val: 0.7967 time: 0.0120s\n",
      "Epoch: 0122 loss_train: 0.5700 acc_train: 0.8857 loss_val: 0.8431 acc_val: 0.7967 time: 0.0090s\n",
      "Epoch: 0123 loss_train: 0.6033 acc_train: 0.8786 loss_val: 0.8385 acc_val: 0.8033 time: 0.0110s\n",
      "Epoch: 0124 loss_train: 0.6094 acc_train: 0.8786 loss_val: 0.8343 acc_val: 0.8067 time: 0.0100s\n",
      "Epoch: 0125 loss_train: 0.5856 acc_train: 0.8643 loss_val: 0.8315 acc_val: 0.8067 time: 0.0100s\n",
      "Epoch: 0126 loss_train: 0.5813 acc_train: 0.9000 loss_val: 0.8286 acc_val: 0.8067 time: 0.0110s\n",
      "Epoch: 0127 loss_train: 0.6138 acc_train: 0.8786 loss_val: 0.8267 acc_val: 0.8067 time: 0.0100s\n",
      "Epoch: 0128 loss_train: 0.5314 acc_train: 0.9214 loss_val: 0.8249 acc_val: 0.8033 time: 0.0110s\n",
      "Epoch: 0129 loss_train: 0.6113 acc_train: 0.8857 loss_val: 0.8227 acc_val: 0.8033 time: 0.0110s\n",
      "Epoch: 0130 loss_train: 0.5518 acc_train: 0.9000 loss_val: 0.8204 acc_val: 0.8033 time: 0.0120s\n",
      "Epoch: 0131 loss_train: 0.5711 acc_train: 0.9000 loss_val: 0.8179 acc_val: 0.8033 time: 0.0120s\n",
      "Epoch: 0132 loss_train: 0.5506 acc_train: 0.9071 loss_val: 0.8146 acc_val: 0.8033 time: 0.0120s\n",
      "Epoch: 0133 loss_train: 0.5445 acc_train: 0.9143 loss_val: 0.8107 acc_val: 0.8033 time: 0.0110s\n",
      "Epoch: 0134 loss_train: 0.5536 acc_train: 0.8857 loss_val: 0.8071 acc_val: 0.8033 time: 0.0120s\n",
      "Epoch: 0135 loss_train: 0.5857 acc_train: 0.8857 loss_val: 0.8036 acc_val: 0.8033 time: 0.0110s\n",
      "Epoch: 0136 loss_train: 0.5239 acc_train: 0.9143 loss_val: 0.8007 acc_val: 0.8100 time: 0.0120s\n",
      "Epoch: 0137 loss_train: 0.4974 acc_train: 0.8929 loss_val: 0.7985 acc_val: 0.8133 time: 0.0100s\n",
      "Epoch: 0138 loss_train: 0.5362 acc_train: 0.9214 loss_val: 0.7970 acc_val: 0.8100 time: 0.0100s\n",
      "Epoch: 0139 loss_train: 0.5063 acc_train: 0.9143 loss_val: 0.7958 acc_val: 0.8100 time: 0.0110s\n",
      "Epoch: 0140 loss_train: 0.5062 acc_train: 0.9286 loss_val: 0.7938 acc_val: 0.8067 time: 0.0100s\n",
      "Epoch: 0141 loss_train: 0.5622 acc_train: 0.9000 loss_val: 0.7913 acc_val: 0.8067 time: 0.0100s\n",
      "Epoch: 0142 loss_train: 0.5367 acc_train: 0.8857 loss_val: 0.7884 acc_val: 0.8067 time: 0.0100s\n",
      "Epoch: 0143 loss_train: 0.5296 acc_train: 0.9143 loss_val: 0.7852 acc_val: 0.8067 time: 0.0100s\n",
      "Epoch: 0144 loss_train: 0.5162 acc_train: 0.9000 loss_val: 0.7827 acc_val: 0.8033 time: 0.0110s\n",
      "Epoch: 0145 loss_train: 0.5227 acc_train: 0.8929 loss_val: 0.7797 acc_val: 0.7967 time: 0.0110s\n",
      "Epoch: 0146 loss_train: 0.5627 acc_train: 0.8857 loss_val: 0.7767 acc_val: 0.8033 time: 0.0100s\n",
      "Epoch: 0147 loss_train: 0.5465 acc_train: 0.9000 loss_val: 0.7741 acc_val: 0.8100 time: 0.0110s\n",
      "Epoch: 0148 loss_train: 0.5047 acc_train: 0.9000 loss_val: 0.7732 acc_val: 0.8100 time: 0.0110s\n",
      "Epoch: 0149 loss_train: 0.5149 acc_train: 0.9071 loss_val: 0.7724 acc_val: 0.8033 time: 0.0110s\n",
      "Epoch: 0150 loss_train: 0.5049 acc_train: 0.9143 loss_val: 0.7720 acc_val: 0.8067 time: 0.0110s\n",
      "Epoch: 0151 loss_train: 0.5220 acc_train: 0.8786 loss_val: 0.7714 acc_val: 0.8067 time: 0.0110s\n",
      "Epoch: 0152 loss_train: 0.5189 acc_train: 0.8857 loss_val: 0.7703 acc_val: 0.8033 time: 0.0100s\n",
      "Epoch: 0153 loss_train: 0.4542 acc_train: 0.9286 loss_val: 0.7685 acc_val: 0.8000 time: 0.0108s\n",
      "Epoch: 0154 loss_train: 0.4796 acc_train: 0.9143 loss_val: 0.7674 acc_val: 0.8000 time: 0.0100s\n",
      "Epoch: 0155 loss_train: 0.4850 acc_train: 0.9214 loss_val: 0.7669 acc_val: 0.8100 time: 0.0110s\n",
      "Epoch: 0156 loss_train: 0.4849 acc_train: 0.9357 loss_val: 0.7659 acc_val: 0.8133 time: 0.0100s\n",
      "Epoch: 0157 loss_train: 0.4808 acc_train: 0.9071 loss_val: 0.7651 acc_val: 0.8133 time: 0.0120s\n",
      "Epoch: 0158 loss_train: 0.4725 acc_train: 0.9143 loss_val: 0.7633 acc_val: 0.8133 time: 0.0110s\n",
      "Epoch: 0159 loss_train: 0.4997 acc_train: 0.9000 loss_val: 0.7612 acc_val: 0.8167 time: 0.0110s\n",
      "Epoch: 0160 loss_train: 0.4797 acc_train: 0.9143 loss_val: 0.7591 acc_val: 0.8167 time: 0.0100s\n",
      "Epoch: 0161 loss_train: 0.4667 acc_train: 0.9357 loss_val: 0.7563 acc_val: 0.8167 time: 0.0120s\n",
      "Epoch: 0162 loss_train: 0.4461 acc_train: 0.9143 loss_val: 0.7536 acc_val: 0.8133 time: 0.0100s\n",
      "Epoch: 0163 loss_train: 0.4844 acc_train: 0.8929 loss_val: 0.7509 acc_val: 0.8100 time: 0.0100s\n",
      "Epoch: 0164 loss_train: 0.4634 acc_train: 0.9429 loss_val: 0.7491 acc_val: 0.8067 time: 0.0090s\n",
      "Epoch: 0165 loss_train: 0.4802 acc_train: 0.9429 loss_val: 0.7468 acc_val: 0.8067 time: 0.0100s\n",
      "Epoch: 0166 loss_train: 0.4868 acc_train: 0.9143 loss_val: 0.7457 acc_val: 0.8033 time: 0.0110s\n",
      "Epoch: 0167 loss_train: 0.5276 acc_train: 0.9143 loss_val: 0.7449 acc_val: 0.8067 time: 0.0120s\n",
      "Epoch: 0168 loss_train: 0.4979 acc_train: 0.9071 loss_val: 0.7446 acc_val: 0.8133 time: 0.0110s\n",
      "Epoch: 0169 loss_train: 0.4395 acc_train: 0.9643 loss_val: 0.7446 acc_val: 0.8133 time: 0.0110s\n",
      "Epoch: 0170 loss_train: 0.4690 acc_train: 0.9143 loss_val: 0.7454 acc_val: 0.8133 time: 0.0110s\n",
      "Epoch: 0171 loss_train: 0.4146 acc_train: 0.9357 loss_val: 0.7459 acc_val: 0.8133 time: 0.0120s\n",
      "Epoch: 0172 loss_train: 0.4564 acc_train: 0.9214 loss_val: 0.7453 acc_val: 0.8133 time: 0.0110s\n",
      "Epoch: 0173 loss_train: 0.4441 acc_train: 0.9286 loss_val: 0.7430 acc_val: 0.8133 time: 0.0110s\n",
      "Epoch: 0174 loss_train: 0.3994 acc_train: 0.9500 loss_val: 0.7397 acc_val: 0.8133 time: 0.0110s\n",
      "Epoch: 0175 loss_train: 0.4660 acc_train: 0.9071 loss_val: 0.7363 acc_val: 0.8133 time: 0.0110s\n",
      "Epoch: 0176 loss_train: 0.4514 acc_train: 0.9286 loss_val: 0.7340 acc_val: 0.8100 time: 0.0110s\n",
      "Epoch: 0177 loss_train: 0.4591 acc_train: 0.9357 loss_val: 0.7322 acc_val: 0.8100 time: 0.0110s\n",
      "Epoch: 0178 loss_train: 0.4362 acc_train: 0.9571 loss_val: 0.7304 acc_val: 0.8133 time: 0.0120s\n",
      "Epoch: 0179 loss_train: 0.4411 acc_train: 0.9357 loss_val: 0.7293 acc_val: 0.8133 time: 0.0110s\n",
      "Epoch: 0180 loss_train: 0.4309 acc_train: 0.9429 loss_val: 0.7287 acc_val: 0.8133 time: 0.0100s\n",
      "Epoch: 0181 loss_train: 0.4403 acc_train: 0.9214 loss_val: 0.7278 acc_val: 0.8133 time: 0.0110s\n",
      "Epoch: 0182 loss_train: 0.4335 acc_train: 0.9286 loss_val: 0.7275 acc_val: 0.8133 time: 0.0110s\n",
      "Epoch: 0183 loss_train: 0.4442 acc_train: 0.9429 loss_val: 0.7280 acc_val: 0.8133 time: 0.0120s\n",
      "Epoch: 0184 loss_train: 0.4320 acc_train: 0.9429 loss_val: 0.7270 acc_val: 0.8067 time: 0.0110s\n",
      "Epoch: 0185 loss_train: 0.3877 acc_train: 0.9429 loss_val: 0.7256 acc_val: 0.8033 time: 0.0120s\n",
      "Epoch: 0186 loss_train: 0.4511 acc_train: 0.9286 loss_val: 0.7248 acc_val: 0.8067 time: 0.0100s\n",
      "Epoch: 0187 loss_train: 0.4331 acc_train: 0.9286 loss_val: 0.7250 acc_val: 0.8033 time: 0.0090s\n",
      "Epoch: 0188 loss_train: 0.4192 acc_train: 0.9429 loss_val: 0.7232 acc_val: 0.8033 time: 0.0110s\n",
      "Epoch: 0189 loss_train: 0.4046 acc_train: 0.9714 loss_val: 0.7209 acc_val: 0.8033 time: 0.0100s\n",
      "Epoch: 0190 loss_train: 0.4090 acc_train: 0.9286 loss_val: 0.7195 acc_val: 0.8033 time: 0.0100s\n",
      "Epoch: 0191 loss_train: 0.4048 acc_train: 0.9571 loss_val: 0.7174 acc_val: 0.8067 time: 0.0100s\n",
      "Epoch: 0192 loss_train: 0.4332 acc_train: 0.9357 loss_val: 0.7150 acc_val: 0.8067 time: 0.0110s\n",
      "Epoch: 0193 loss_train: 0.4034 acc_train: 0.9286 loss_val: 0.7125 acc_val: 0.8067 time: 0.0110s\n",
      "Epoch: 0194 loss_train: 0.4408 acc_train: 0.9286 loss_val: 0.7106 acc_val: 0.8033 time: 0.0100s\n",
      "Epoch: 0195 loss_train: 0.3998 acc_train: 0.9429 loss_val: 0.7081 acc_val: 0.8033 time: 0.0100s\n",
      "Epoch: 0196 loss_train: 0.4377 acc_train: 0.9500 loss_val: 0.7062 acc_val: 0.8067 time: 0.0110s\n",
      "Epoch: 0197 loss_train: 0.4358 acc_train: 0.9143 loss_val: 0.7053 acc_val: 0.8067 time: 0.0100s\n",
      "Epoch: 0198 loss_train: 0.4153 acc_train: 0.9357 loss_val: 0.7050 acc_val: 0.8067 time: 0.0110s\n",
      "Epoch: 0199 loss_train: 0.4109 acc_train: 0.9214 loss_val: 0.7055 acc_val: 0.8067 time: 0.0130s\n",
      "Epoch: 0200 loss_train: 0.3899 acc_train: 0.9286 loss_val: 0.7067 acc_val: 0.8100 time: 0.0120s\n",
      "Optimization Finished!\n",
      "Total time elapsed: 2.3582s\n",
      "Test set results: loss= 0.7303 accuracy= 0.8230\n"
     ]
    }
   ],
   "source": [
    "# train.py\n",
    "\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "# Training settings\n",
    "no_cuda = False\n",
    "fastmode = False\n",
    "seed = 42\n",
    "epochs = 200\n",
    "lr = 0.01\n",
    "weight_decay = 5e-4\n",
    "hidden = 16\n",
    "dropout = 0.5\n",
    "\n",
    "cuda = not no_cuda and torch.cuda.is_available()\n",
    "\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if cuda:\n",
    "    torch.cuda.manual_seed(seed)\n",
    "\n",
    "# Load data\n",
    "adj, features, labels, idx_train, idx_val, idx_test = load_data()\n",
    "\n",
    "# Model and optimizer\n",
    "model = GCN(nfeat=features.shape[1],\n",
    "            nhid=hidden,\n",
    "            nclass=labels.max().item() + 1,\n",
    "            dropout=dropout)\n",
    "optimizer = optim.Adam(model.parameters(),\n",
    "                       lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "if cuda:\n",
    "    model.cuda()\n",
    "    features = features.cuda()\n",
    "    adj = adj.cuda()\n",
    "    labels = labels.cuda()\n",
    "    idx_train = idx_train.cuda()\n",
    "    idx_val = idx_val.cuda()\n",
    "    idx_test = idx_test.cuda()\n",
    "\n",
    "\n",
    "def train(epoch):\n",
    "    t = time.time()\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(features, adj)\n",
    "    loss_train = F.nll_loss(output[idx_train], labels[idx_train])\n",
    "    acc_train = accuracy(output[idx_train], labels[idx_train])\n",
    "    loss_train.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if not fastmode:\n",
    "        # Evaluate validation set performance separately,\n",
    "        # deactivates dropout during validation run.\n",
    "        model.eval()\n",
    "        output = model(features, adj)\n",
    "\n",
    "    loss_val = F.nll_loss(output[idx_val], labels[idx_val])\n",
    "    acc_val = accuracy(output[idx_val], labels[idx_val])\n",
    "    print('Epoch: {:04d}'.format(epoch+1),\n",
    "          'loss_train: {:.4f}'.format(loss_train.item()),\n",
    "          'acc_train: {:.4f}'.format(acc_train.item()),\n",
    "          'loss_val: {:.4f}'.format(loss_val.item()),\n",
    "          'acc_val: {:.4f}'.format(acc_val.item()),\n",
    "          'time: {:.4f}s'.format(time.time() - t))\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    output = model(features, adj)\n",
    "    loss_test = F.nll_loss(output[idx_test], labels[idx_test])\n",
    "    acc_test = accuracy(output[idx_test], labels[idx_test])\n",
    "    print(\"Test set results:\",\n",
    "          \"loss= {:.4f}\".format(loss_test.item()),\n",
    "          \"accuracy= {:.4f}\".format(acc_test.item()))\n",
    "\n",
    "# Train model\n",
    "t_total = time.time()\n",
    "for epoch in range(epochs):\n",
    "    train(epoch)\n",
    "print(\"Optimization Finished!\")\n",
    "print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))\n",
    "\n",
    "# Testing\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dgl.data.utils import save_graphs\r\n",
    "\r\n",
    "torch.save(model.state_dict(), \"./model.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{2157661694208: 'conv1.weight', 2157661695488: 'bn1.weight', 2157661695616: 'bn1.bias', 2157665134720: 'bn1.running_mean', 2157665134272: 'bn1.running_var', 2157665133440: 'bn1.num_batches_tracked', 2157665136064: 'layer1.0.conv1.weight', 2157665134848: 'layer1.0.bn1.weight', 2157665136512: 'layer1.0.bn1.bias', 2157665132992: 'layer1.0.bn1.running_mean', 2157665132800: 'layer1.0.bn1.running_var', 2157665132928: 'layer1.0.bn1.num_batches_tracked', 2157665119232: 'layer1.0.conv2.weight', 2157665118912: 'layer1.0.bn2.weight', 2157665118976: 'layer1.0.bn2.bias', 2157665117632: 'layer1.0.bn2.running_mean', 2157665117120: 'layer1.0.bn2.running_var', 2157665118400: 'layer1.0.bn2.num_batches_tracked', 2157665116928: 'layer1.1.conv1.weight', 2157665119680: 'layer1.1.bn1.weight', 2157665117568: 'layer1.1.bn1.bias', 2157665126720: 'layer1.1.bn1.running_mean', 2157665126848: 'layer1.1.bn1.running_var', 2157665127872: 'layer1.1.bn1.num_batches_tracked', 2157665127232: 'layer1.1.conv2.weight', 2157665125504: 'layer1.1.bn2.weight', 2157665126784: 'layer1.1.bn2.bias', 2157665124800: 'layer1.1.bn2.running_mean', 2157665126592: 'layer1.1.bn2.running_var', 2157665125312: 'layer1.1.bn2.num_batches_tracked', 2157665125760: 'layer2.0.conv1.weight', 2157662410240: 'layer2.0.bn1.weight', 2157662410880: 'layer2.0.bn1.bias', 2157662409280: 'layer2.0.bn1.running_mean', 2157662412160: 'layer2.0.bn1.running_var', 2157662409344: 'layer2.0.bn1.num_batches_tracked', 2157662412288: 'layer2.0.conv2.weight', 2157662409600: 'layer2.0.bn2.weight', 2157662410944: 'layer2.0.bn2.bias', 2157662411200: 'layer2.0.bn2.running_mean', 2157662409024: 'layer2.0.bn2.running_var', 2157662409920: 'layer2.0.bn2.num_batches_tracked', 2157665096448: 'layer2.0.downsample.0.weight', 2157665097280: 'layer2.0.downsample.1.weight', 2157665099648: 'layer2.0.downsample.1.bias', 2157665096000: 'layer2.0.downsample.1.running_mean', 2157665095744: 'layer2.0.downsample.1.running_var', 2157665097024: 'layer2.0.downsample.1.num_batches_tracked', 2157665099136: 'layer2.1.conv1.weight', 2157665098304: 'layer2.1.bn1.weight', 2157665096064: 'layer2.1.bn1.bias', 2157665098624: 'layer2.1.bn1.running_mean', 2157665098880: 'layer2.1.bn1.running_var', 2157665096768: 'layer2.1.bn1.num_batches_tracked', 2157665104896: 'layer2.1.conv2.weight', 2157665105664: 'layer2.1.bn2.weight', 2157665106304: 'layer2.1.bn2.bias', 2157665104320: 'layer2.1.bn2.running_mean', 2157665107520: 'layer2.1.bn2.running_var', 2157665107264: 'layer2.1.bn2.num_batches_tracked', 2157665106432: 'layer3.0.conv1.weight', 2157665104640: 'layer3.0.bn1.weight', 2157665107648: 'layer3.0.bn1.bias', 2157665104128: 'layer3.0.bn1.running_mean', 2157665107136: 'layer3.0.bn1.running_var', 2157665084352: 'layer3.0.bn1.num_batches_tracked', 2157665086016: 'layer3.0.conv2.weight', 2157665084096: 'layer3.0.bn2.weight', 2157665084480: 'layer3.0.bn2.bias', 2157665085440: 'layer3.0.bn2.running_mean', 2157665084032: 'layer3.0.bn2.running_var', 2157665084992: 'layer3.0.bn2.num_batches_tracked', 2157665083584: 'layer3.0.downsample.0.weight', 2157665086272: 'layer3.0.downsample.1.weight', 2157665085312: 'layer3.0.downsample.1.bias', 2157665087360: 'layer3.0.downsample.1.running_mean', 2157665086784: 'layer3.0.downsample.1.running_var', 2157665087104: 'layer3.0.downsample.1.num_batches_tracked', 2157665084672: 'layer3.1.conv1.weight', 2157665086336: 'layer3.1.bn1.weight', 2157665086400: 'layer3.1.bn1.bias', 2157665141952: 'layer3.1.bn1.running_mean', 2157665144128: 'layer3.1.bn1.running_var', 2157665143616: 'layer3.1.bn1.num_batches_tracked', 2157665075136: 'layer3.1.conv2.weight', 2157665071424: 'layer3.1.bn2.weight', 2157665074048: 'layer3.1.bn2.bias', 2157665073920: 'layer3.1.bn2.running_mean', 2157665075072: 'layer3.1.bn2.running_var', 2157665074688: 'layer3.1.bn2.num_batches_tracked', 2157665071936: 'layer4.0.conv1.weight', 2157665074752: 'layer4.0.bn1.weight', 2157665072448: 'layer4.0.bn1.bias', 2157665071744: 'layer4.0.bn1.running_mean', 2157665073728: 'layer4.0.bn1.running_var', 2157665072384: 'layer4.0.bn1.num_batches_tracked', 2157662549248: 'layer4.0.conv2.weight', 2157662549056: 'layer4.0.bn2.weight', 2157662549632: 'layer4.0.bn2.bias', 2157662549312: 'layer4.0.bn2.running_mean', 2157662551360: 'layer4.0.bn2.running_var', 2157662551424: 'layer4.0.bn2.num_batches_tracked', 2157662549952: 'layer4.0.downsample.0.weight', 2157662550656: 'layer4.0.downsample.1.weight', 2157662551808: 'layer4.0.downsample.1.bias', 2157662551616: 'layer4.0.downsample.1.running_mean', 2157662550144: 'layer4.0.downsample.1.running_var', 2157662971648: 'layer4.0.downsample.1.num_batches_tracked', 2157662969984: 'layer4.1.conv1.weight', 2157662971200: 'layer4.1.bn1.weight', 2157662971456: 'layer4.1.bn1.bias', 2157662971584: 'layer4.1.bn1.running_mean', 2157662971712: 'layer4.1.bn1.running_var', 2157662971968: 'layer4.1.bn1.num_batches_tracked', 2157662973696: 'layer4.1.conv2.weight', 2157662971840: 'layer4.1.bn2.weight', 2157662971072: 'layer4.1.bn2.bias', 2157662970432: 'layer4.1.bn2.running_mean', 2157662972480: 'layer4.1.bn2.running_var', 2157662972160: 'layer4.1.bn2.num_batches_tracked', 2157666235584: 'fc.weight', 2157666236288: 'fc.bias'}\n"
     ]
    },
    {
     "ename": "ExecutableNotFound",
     "evalue": "failed to execute ['dot', '-Kdot', '-Tpdf', '-O', 'Digraph.gv'], make sure the Graphviz executables are on your systems' PATH",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\graphviz\\backend.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(cmd, input, capture_output, check, encoding, quiet, **kwargs)\u001b[0m\n\u001b[0;32m    163\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 164\u001b[1;33m         \u001b[0mproc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msubprocess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstartupinfo\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mget_startupinfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    165\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\subprocess.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, encoding, errors, text)\u001b[0m\n\u001b[0;32m    853\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 854\u001b[1;33m             self._execute_child(args, executable, preexec_fn, close_fds,\n\u001b[0m\u001b[0;32m    855\u001b[0m                                 \u001b[0mpass_fds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcwd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\subprocess.py\u001b[0m in \u001b[0;36m_execute_child\u001b[1;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, unused_restore_signals, unused_start_new_session)\u001b[0m\n\u001b[0;32m   1306\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1307\u001b[1;33m                 hp, ht, pid, tid = _winapi.CreateProcess(executable, args,\n\u001b[0m\u001b[0;32m   1308\u001b[0m                                          \u001b[1;31m# no special security\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 2] 지정된 파일을 찾을 수 없습니다",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mExecutableNotFound\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-882c990dd259>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[0mg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmake_dot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresnet18\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 62\u001b[1;33m \u001b[0mg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\graphviz\\files.py\u001b[0m in \u001b[0;36mview\u001b[1;34m(self, filename, directory, cleanup, quiet, quiet_view)\u001b[0m\n\u001b[0;32m    280\u001b[0m             \u001b[0mto\u001b[0m \u001b[0mretrieve\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mapplication\u001b[0m\u001b[0;31m'\u001b[0m\u001b[0ms\u001b[0m \u001b[0mexit\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    281\u001b[0m         \"\"\"\n\u001b[1;32m--> 282\u001b[1;33m         return self.render(filename=filename, directory=directory,\n\u001b[0m\u001b[0;32m    283\u001b[0m                            \u001b[0mview\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcleanup\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcleanup\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    284\u001b[0m                            quiet=quiet, quiet_view=quiet_view)\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\graphviz\\files.py\u001b[0m in \u001b[0;36mrender\u001b[1;34m(self, filename, directory, view, cleanup, format, renderer, formatter, quiet, quiet_view)\u001b[0m\n\u001b[0;32m    241\u001b[0m             \u001b[0mformat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_format\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    242\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 243\u001b[1;33m         rendered = backend.render(self._engine, format, filepath,\n\u001b[0m\u001b[0;32m    244\u001b[0m                                   \u001b[0mrenderer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mformatter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mformatter\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    245\u001b[0m                                   quiet=quiet)\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\graphviz\\backend.py\u001b[0m in \u001b[0;36mrender\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m    221\u001b[0m         \u001b[0mcwd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    222\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 223\u001b[1;33m     \u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcapture_output\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcwd\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcwd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheck\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mquiet\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mquiet\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    224\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mrendered\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    225\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\graphviz\\backend.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(cmd, input, capture_output, check, encoding, quiet, **kwargs)\u001b[0m\n\u001b[0;32m    165\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    166\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrno\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0merrno\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mENOENT\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 167\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mExecutableNotFound\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    168\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    169\u001b[0m             \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mExecutableNotFound\u001b[0m: failed to execute ['dot', '-Kdot', '-Tpdf', '-O', 'Digraph.gv'], make sure the Graphviz executables are on your systems' PATH"
     ]
    }
   ],
   "source": [
    "from graphviz import Digraph\r\n",
    "import torch\r\n",
    "from torch.autograd import Variable\r\n",
    "\r\n",
    "\r\n",
    "def make_dot(var, params):\r\n",
    "    \"\"\" Produces Graphviz representation of PyTorch autograd graph\r\n",
    "    \r\n",
    "    Blue nodes are the Variables that require grad, orange are Tensors\r\n",
    "    saved for backward in torch.autograd.Function\r\n",
    "    \r\n",
    "    Args:\r\n",
    "        var: output Variable\r\n",
    "        params: dict of (name, Variable) to add names to node that\r\n",
    "            require grad (TODO: make optional)\r\n",
    "    \"\"\"\r\n",
    "    param_map = {id(v): k for k, v in params.items()}\r\n",
    "    print(param_map)\r\n",
    "    \r\n",
    "    node_attr = dict(style='filled',\r\n",
    "                     shape='box',\r\n",
    "                     align='left',\r\n",
    "                     fontsize='12',\r\n",
    "                     ranksep='0.1',\r\n",
    "                     height='0.2')\r\n",
    "    dot = Digraph(node_attr=node_attr, graph_attr=dict(size=\"12,12\"))\r\n",
    "    seen = set()\r\n",
    "    \r\n",
    "    def size_to_str(size):\r\n",
    "        return '('+(', ').join(['%d'% v for v in size])+')'\r\n",
    "\r\n",
    "    def add_nodes(var):\r\n",
    "        if var not in seen:\r\n",
    "            if torch.is_tensor(var):\r\n",
    "                dot.node(str(id(var)), size_to_str(var.size()), fillcolor='orange')\r\n",
    "            elif hasattr(var, 'variable'):\r\n",
    "                u = var.variable\r\n",
    "                node_name = '%s\\n %s' % (param_map.get(id(u)), size_to_str(u.size()))\r\n",
    "                dot.node(str(id(var)), node_name, fillcolor='lightblue')\r\n",
    "            else:\r\n",
    "                dot.node(str(id(var)), str(type(var).__name__))\r\n",
    "            seen.add(var)\r\n",
    "            if hasattr(var, 'next_functions'):\r\n",
    "                for u in var.next_functions:\r\n",
    "                    if u[0] is not None:\r\n",
    "                        dot.edge(str(id(u[0])), str(id(var)))\r\n",
    "                        add_nodes(u[0])\r\n",
    "            if hasattr(var, 'saved_tensors'):\r\n",
    "                for t in var.saved_tensors:\r\n",
    "                    dot.edge(str(id(t)), str(id(var)))\r\n",
    "                    add_nodes(t)\r\n",
    "    add_nodes(var.grad_fn)\r\n",
    "    return dot\r\n",
    "\r\n",
    "from torchvision import models\r\n",
    "inputs = torch.randn(1,3,224,224)\r\n",
    "resnet18 = models.resnet18()\r\n",
    "y = resnet18(Variable(inputs))\r\n",
    "# print(y)\r\n",
    "\r\n",
    "g = make_dot(y, resnet18.state_dict())\r\n",
    "g.view()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting graphviz\n",
      "  Downloading graphviz-0.16-py2.py3-none-any.whl (19 kB)\n",
      "Installing collected packages: graphviz\n",
      "Successfully installed graphviz-0.16\n"
     ]
    }
   ],
   "source": [
    "!pip install graphviz"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "name": "python385jvsc74a57bd0c686f9301b12c31c14e82dc29ca863532bd7f873f06cfd9cdb9caff7ec0d55c0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}